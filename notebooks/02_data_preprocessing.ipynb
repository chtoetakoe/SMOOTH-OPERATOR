{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Notebook 02 â€” Data Preprocessing & Feature Engineering\n",
    "\n",
    "\n",
    "\n",
    "**Objective:** clean the base dataset and engineer features for analysis and machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "| Step | Description | Output |\n",
    "|------|-------------|--------|\n",
    "| 1 | Load base dataset from Notebook 01 | Raw merged data |\n",
    "| 2 | Clean qualifying grid positions | `grid_clean` column |\n",
    "| 3 | Create race-level features | `position_gain`, `is_podium` |\n",
    "| 4 | Add finish status flags | `is_finished`, `is_dnf`, `is_dns` |\n",
    "| 5 | Engineer historical features | Driver & constructor past performance |\n",
    "| 6 | Final cleaning & validation | Handle remaining missing values |\n",
    "\n",
    "### Output\n",
    "\n",
    " saved to: `../data/processed/processed_f1_2018_2024.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Setup & Imports](#1-setup)\n",
    "2. [Load Base Data](#2-load)\n",
    "3. [Initial Data Quality](#3-quality)\n",
    "4. [Grid Cleaning](#4-grid)\n",
    "5. [Race Features](#5-race-features)\n",
    "6. [Status & DNF Flags](#6-status)\n",
    "7. [Historical Features](#7-historical)\n",
    "8. [Final Cleaning](#8-final-clean)\n",
    "9. [Validation & Verification](#9-validation)\n",
    "10. [Save & Summary](#10-save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup & Imports <a id='1-setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if not (PROJECT_ROOT / \"src\").exists() and (PROJECT_ROOT.parent / \"src\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "#import custom preprocessing functions\n",
    "from src.data_processing import (\n",
    "    clean_grid,\n",
    "    add_race_features,\n",
    "    attach_status_text,\n",
    "    add_dnf_dns_flags,\n",
    "    add_time_aware_aggregates,\n",
    "    final_clean,\n",
    ")\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_RAW = PROJECT_ROOT / 'data' / 'raw'\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed'\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_PATH = DATA_PROCESSED / 'f1_base_2018_2024.csv'\n",
    "FINAL_PATH = DATA_PROCESSED / 'processed_f1_2018_2024.csv'\n",
    "\n",
    "print(f\"Input:  {BASE_PATH}\")\n",
    "print(f\"Output: {FINAL_PATH}\")\n",
    "print(f\"Input exists: {BASE_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Load Base Data <a id='2-load'></a>\n",
    "\n",
    "Load the base dataset created in Notebook 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading base dataset\n",
    "df = pd.read_csv(BASE_PATH, parse_dates=['date'])\n",
    "\n",
    "print(f\"Base dataset loaded!\")\n",
    "print(f\"Shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "print(f\"Years: {df['year'].min()} - {df['year'].max()}\")\n",
    "print(f\"Memory: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preview-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "columns-list",
   "metadata": {},
   "outputs": [],
   "source": [
    "#current columns\n",
    "print(\"Current columns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Initial Data Quality <a id='3-quality'></a>\n",
    "\n",
    "Assess data quality before any transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-before",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values BEFORE cleaning\n",
    "print(\"MISSING VALUES (Before Cleaning)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_before = df.isna().sum()\n",
    "missing_pct = (missing_before / len(df) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_before,\n",
    "    'Missing %': missing_pct\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "display(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "if missing_df['Missing Count'].sum() == 0:\n",
    "    print(\"No missing values in the dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats-before",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Key statistics BEFORE cleaning\n",
    "print(\"KEY STATISTICS (Before Cleaning)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "display(df[['grid', 'positionOrder', 'points']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grid-zero-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check grid=0 values(special case in Ergast data)\n",
    "grid_zero_count = (df['grid'] == 0).sum()\n",
    "print(f\"Grid position = 0 (pit lane start / unknown): {grid_zero_count} rows\")\n",
    "print(f\"This represents {grid_zero_count / len(df) * 100:.2f}% of data\")\n",
    "print()\n",
    "print(\"These will be treated as missing (NaN) in grid_clean column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Grid Cleaning <a id='4-grid'></a>\n",
    "\n",
    "### Problem\n",
    "In F1 data, `grid = 0` doesn't mean pole position. It indicates:\n",
    "- Pit lane start\n",
    "- Unknown qualifying position\n",
    "- Disqualification from qualifying\n",
    "\n",
    "### Solution\n",
    "Create `grid_clean` column where `grid = 0` is replaced with `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-grid",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply grid cleaning\n",
    "df = clean_grid(df)\n",
    "\n",
    "print(\"Grid cleaning applied!\")\n",
    "print()\n",
    "print(\"Comparison:\")\n",
    "print(f\"   grid values:       min={df['grid'].min()}, max={df['grid'].max()}\")\n",
    "print(f\"   grid_clean values: min={df['grid_clean'].min()}, max={df['grid_clean'].max()}\")\n",
    "print(f\"   grid_clean NaN:    {df['grid_clean'].isna().sum()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-grid",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show rows where grid=0 became NaN\n",
    "print(\"Sample rows where grid=0 (now grid_clean=NaN):\")\n",
    "df.loc[df['grid'] == 0, ['driverName', 'name', 'year', 'grid', 'grid_clean', 'positionOrder', 'points']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Race Features <a id='5-race-features'></a>\n",
    "\n",
    "Create derived features for each race result:\n",
    "\n",
    "| Feature | Formula | Description |\n",
    "|---------|---------|-------------|\n",
    "| `position_gain` | grid_clean - positionOrder | Positive = gained positions |\n",
    "| `is_podium` | positionOrder <= 3 | Binary: finished top 3 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-race-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add race features\n",
    "df = add_race_features(df)\n",
    "\n",
    "print(\"Race features added!\")\n",
    "print()\n",
    "print(\"New columns: position_gain, is_podium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-race-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify race features\n",
    "print(\"RACE FEATURES VERIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "df[['driverName', 'name', 'grid_clean', 'positionOrder', 'position_gain', 'points', 'is_podium']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "position-gain-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "#position gain statistics\n",
    "print(\"POSITION GAIN STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean position gain:  {df['position_gain'].mean():.2f}\")\n",
    "print(f\"Max positions gained: {df['position_gain'].max():.0f}\")\n",
    "print(f\"Max positions lost:   {df['position_gain'].min():.0f}\")\n",
    "print()\n",
    "print(f\"Podium finishes: {df['is_podium'].sum()} ({df['is_podium'].mean()*100:.1f}% of races)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Status & DNF Flags <a id='6-status'></a>\n",
    "\n",
    "Create flags to identify race finish status:\n",
    "\n",
    "| Flag | Description |\n",
    "|------|-------------|\n",
    "| `is_finished` | Driver completed the race |\n",
    "| `is_dnf` | Did Not Finish (mechanical, crash, etc.) |\n",
    "| `is_dns` | Did Not Start |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attach status text if available\n",
    "status_path = DATA_RAW / 'status.csv'\n",
    "\n",
    "if status_path.exists() and 'statusId' in df.columns:\n",
    "    status_df = pd.read_csv(status_path)\n",
    "    df = attach_status_text(df, status_df)\n",
    "    print(f\"Status text attached from {status_path.name}\")\n",
    "else:\n",
    "    print(\"Status file not found or statusId not in dataframe.\")\n",
    "\n",
    "#add DNF/DNS flags\n",
    "df = add_dnf_dns_flags(df)\n",
    "print(\"DNF/DNS flags added!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify status flags\n",
    "print(\"STATUS FLAGS VERIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "status_cols = [c for c in ['statusId', 'status_text', 'is_finished', 'is_dnf', 'is_dns'] if c in df.columns]\n",
    "if status_cols:\n",
    "    display(df[['driverName', 'name'] + status_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "status-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#status summary\n",
    "print(\"STATUS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Finished:       {df['is_finished'].sum():,} ({df['is_finished'].mean()*100:.1f}%)\")\n",
    "print(f\"DNF:            {df['is_dnf'].sum():,} ({df['is_dnf'].mean()*100:.1f}%)\")\n",
    "print(f\"DNS:            {df['is_dns'].sum():,} ({df['is_dns'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Historical Features <a id='7-historical'></a>\n",
    "\n",
    "Engineer time-aware features using **only past data** to prevent data leakage.\n",
    "\n",
    "### Driver Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| `driver_races_past` | Number of races before current race |\n",
    "| `driver_avg_points_past` | Average points in previous races |\n",
    "| `driver_consistency_past` | Std dev of finish positions (lower = more consistent) |\n",
    "\n",
    "### Constructor Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| `constructor_races_past` | Number of races before current race |\n",
    "| `constructor_strength_past` | Team's average points in previous races |\n",
    "| `constructor_avg_finish_past` | Team's average finish position |\n",
    "\n",
    "### Key Technique: `expanding().mean().shift(1)`\n",
    "\n",
    "This ensures we only use **past data**, never the current race's result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-historical",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add time-aware historical features\n",
    "print(\"Adding historical features...\")\n",
    "print(\"(This may take a moment)\")\n",
    "\n",
    "df = add_time_aware_aggregates(df)\n",
    "\n",
    "print(\"\\nHistorical features added!\")\n",
    "print(\"New columns:\")\n",
    "print(\"   - driver_races_past\")\n",
    "print(\"   - driver_avg_points_past\")\n",
    "print(\"   - driver_consistency_past\")\n",
    "print(\"   - constructor_races_past\")\n",
    "print(\"   - constructor_strength_past\")\n",
    "print(\"   - constructor_avg_finish_past\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-historical",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify historical features\n",
    "print(\"HISTORICAL FEATURES VERIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "hist_cols = [\n",
    "    'driverName', 'constructorName', 'date',\n",
    "    'driver_races_past', 'driver_avg_points_past', 'driver_consistency_past',\n",
    "    'constructor_races_past', 'constructor_strength_past', 'constructor_avg_finish_past'\n",
    "]\n",
    "\n",
    "df[hist_cols].head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example:Max Verstappen's historical features over time\n",
    "print(\"EXAMPLE: Max Verstappen's Historical Features (first 10 races in dataset)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "verstappen = df[df['driverName'] == 'Max Verstappen'].sort_values('date').head(10)\n",
    "verstappen[['date', 'name', 'points', 'driver_races_past', 'driver_avg_points_past', 'constructor_strength_past']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Final Cleaning <a id='8-final-clean'></a>\n",
    "\n",
    "Handle any remaining missing values with sensible defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-clean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply final cleaning\n",
    "df = final_clean(df)\n",
    "\n",
    "print(\"Final cleaning applied!\")\n",
    "print()\n",
    "print(\"Missing value handling:\")\n",
    "print(\"   - driver_avg_points_past: NaN -> 0 (new drivers)\")\n",
    "print(\"   - constructor_strength_past: NaN -> 0 (new teams)\")\n",
    "print(\"   - consistency features: NaN -> median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-after",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check missing values AFTER cleaning\n",
    "print(\"MISSING VALUES (After Cleaning)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_after = df.isna().sum()\n",
    "missing_after_df = missing_after[missing_after > 0]\n",
    "\n",
    "if len(missing_after_df) > 0:\n",
    "    print(missing_after_df)\n",
    "else:\n",
    "    print(\"No missing values remaining! (except grid_clean by design)\")\n",
    "    \n",
    "print(f\"\\ngrid_clean NaN count: {df['grid_clean'].isna().sum()} (expected - pit lane starts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Validation & Verification <a id='9-validation'></a>\n",
    "\n",
    "Verify all transformations were applied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "all-columns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns in processed dataset\n",
    "print(\"ALL COLUMNS IN PROCESSED DATASET\")\n",
    "print(\"=\" * 50)\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final statistics\n",
    "print(\"FINAL DATASET STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "display(df[numeric_cols].describe().round(2).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key engineered features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. Position gain distribution\n",
    "axes[0, 0].hist(df['position_gain'].dropna(), bins=30, color='#3498db', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_title('Position Gain Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Position Gain (positive = gained positions)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 2. Driver avg points past\n",
    "axes[0, 1].hist(df['driver_avg_points_past'].dropna(), bins=30, color='#2ecc71', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('Driver Historical Avg Points', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Average Points (past races)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Constructor strength\n",
    "axes[1, 0].hist(df['constructor_strength_past'].dropna(), bins=30, color='#e74c3c', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_title('Constructor Strength Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Team Avg Points (past races)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 4. Driver consistency\n",
    "axes[1, 1].hist(df['driver_consistency_past'].dropna(), bins=30, color='#9b59b6', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_title('Driver Consistency Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Finish Position Std Dev (lower = more consistent)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Save & Summary <a id='10-save'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-final-cols",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select final columns in order\n",
    "final_cols = [\n",
    "    # Identifiers\n",
    "    'raceId', 'year', 'round', 'name', 'date',\n",
    "    # Driver info\n",
    "    'driverId', 'driverName', 'nationality',\n",
    "    # Constructor info\n",
    "    'constructorId', 'constructorName',\n",
    "    # Original results\n",
    "    'grid', 'grid_clean', 'positionOrder', 'points',\n",
    "    # Derived features\n",
    "    'position_gain', 'is_podium',\n",
    "    # Status flags\n",
    "    'is_finished', 'is_dnf', 'is_dns',\n",
    "    # Historical features\n",
    "    'driver_races_past', 'driver_avg_points_past', 'driver_consistency_past',\n",
    "    'constructor_races_past', 'constructor_strength_past', 'constructor_avg_finish_past',\n",
    "]\n",
    "\n",
    "#keep only columns that exist\n",
    "final_cols = [c for c in final_cols if c in df.columns]\n",
    "df_final = df[final_cols].copy()\n",
    "\n",
    "print(f\"Final dataset prepared!\")\n",
    "print(f\"Shape: {df_final.shape[0]:,} rows x {df_final.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to CSV\n",
    "df_final.to_csv(FINAL_PATH, index=False)\n",
    "\n",
    "print(f\"Saved to: {FINAL_PATH}\")\n",
    "print(f\"File size: {FINAL_PATH.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final summary\n",
    "print(\"=\" * 70)\n",
    "print(\"NOTEBOOK 02 COMPLETE - DATA PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"INPUT:\")\n",
    "print(f\"   File: {BASE_PATH.name}\")\n",
    "print(f\"   Rows: {len(pd.read_csv(BASE_PATH)):,}\")\n",
    "print()\n",
    "print(\"TRANSFORMATIONS APPLIED:\")\n",
    "print(\"   1. Grid cleaning (grid=0 -> NaN)\")\n",
    "print(\"   2. Race features: position_gain, is_podium\")\n",
    "print(\"   3. Status flags: is_finished, is_dnf, is_dns\")\n",
    "print(\"   4. Driver historical: races_past, avg_points_past, consistency_past\")\n",
    "print(\"   5. Constructor historical: races_past, strength_past, avg_finish_past\")\n",
    "print(\"   6. Final cleaning: handle remaining NaN values\")\n",
    "print()\n",
    "print(\"OUTPUT:\")\n",
    "print(f\"   File: {FINAL_PATH.name}\")\n",
    "print(f\"   Rows: {df_final.shape[0]:,}\")\n",
    "print(f\"   Columns: {df_final.shape[1]}\")\n",
    "print()\n",
    "print(\"NEW FEATURES CREATED:\")\n",
    "new_features = [\n",
    "    'grid_clean', 'position_gain', 'is_podium',\n",
    "    'is_finished', 'is_dnf', 'is_dns',\n",
    "    'driver_races_past', 'driver_avg_points_past', 'driver_consistency_past',\n",
    "    'constructor_races_past', 'constructor_strength_past', 'constructor_avg_finish_past'\n",
    "]\n",
    "for f in new_features:\n",
    "    if f in df_final.columns:\n",
    "        print(f\"   - {f}\")\n",
    "print()\n",
    "print(\"NEXT STEP:\")\n",
    "print(\"   -> Notebook 03: Exploratory Data Analysis (EDA)\")\n",
    "print()\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
