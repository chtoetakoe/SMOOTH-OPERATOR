{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# Notebook 04 — Machine Learning\n",
    "\n",
    "Build and evaluate machine learning models to predict race points based on qualifying position, driver consistency, and constructor strength.\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "| Aspect | Details |\n",
    "|--------|--------|\n",
    "| **Problem Type** | Regression (predicting continuous points 0-26) |\n",
    "| **Target Variable** | `points` — race points scored |\n",
    "| **Models** | Linear Regression, Decision Tree Regressor |\n",
    "| **Split Strategy** | Time-based (Train: 2018-2022, Test: 2023-2024) |\n",
    "| **Evaluation Metrics** | R², MSE (as required by professor) |\n",
    "\n",
    "---\n",
    "\n",
    "### Why Time-Based Split?\n",
    "\n",
    "**time-based split** is used instead of random split to:\n",
    "1. **Prevent data leakage** — Never train on future data\n",
    "2. **Mimic real forecasting** — Train on past seasons, predict upcoming races\n",
    "3. **Realistic evaluation** — Test how well model generalizes to NEW seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toc",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. [Setup & Imports](#1-setup)\n",
    "2. [Load Data](#2-load-data)\n",
    "3. [Why Regression?](#3-why-regression)\n",
    "4. [Feature Selection](#4-feature-selection)\n",
    "5. [Train/Test Split](#5-split)\n",
    "6. [Build Preprocessor & Models](#6-build-models)\n",
    "7. [Train & Evaluate](#7-train-evaluate)\n",
    "8. [Tune Decision Tree](#8-tune-dt)\n",
    "9. [Results Visualization](#9-visualization)\n",
    "10. [Feature Importance](#10-feature-importance)\n",
    "11. [Conclusions](#11-conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup & Imports <a id='1-setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "#data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#scikit-learn\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if not (PROJECT_ROOT / \"src\").exists() and (PROJECT_ROOT.parent / \"src\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "#Import custom ML functions\n",
    "from src.models import (\n",
    "    build_preprocessor,\n",
    "    time_based_split,\n",
    ")\n",
    "\n",
    "#Create directories\n",
    "REPORT_DIR = PROJECT_ROOT / \"reports\"\n",
    "FIG_DIR = REPORT_DIR / \"figures\"\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#Display settings\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"   Project root: {PROJECT_ROOT}\")\n",
    "print(f\"   Reports dir:  {REPORT_DIR}\")\n",
    "print(f\"   Figures dir:  {FIG_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Load Data <a id='2-load-data'></a>\n",
    "\n",
    "Load the preprocessed dataset created in Notebook 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find processed data file\n",
    "CANDIDATES = [\n",
    "    PROJECT_ROOT / \"data\" / \"processed\" / \"processed_f1_2018_2024.csv\",\n",
    "    PROJECT_ROOT / \"data\" / \"processed\" / \"f1_clean_2018_2024.csv\",\n",
    "    PROJECT_ROOT / \"data\" / \"processed\" / \"processed_f1.csv\",\n",
    "]\n",
    "\n",
    "DATA_PATH = next((p for p in CANDIDATES if p.exists()), None)\n",
    "if DATA_PATH is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find processed CSV. Checked:\\n\" + \"\\n\".join(str(p) for p in CANDIDATES)\n",
    "    )\n",
    "\n",
    "#Load data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"Data loaded from: {DATA_PATH.name}\")\n",
    "print(f\"   Shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "print(f\"   Years: {int(df['year'].min())} - {int(df['year'].max())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preview-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Why Regression? <a id='3-why-regression'></a>\n",
    "\n",
    "Before building models, we need to understand **why we chose Regression** instead of Classification or Clustering.\n",
    "\n",
    "### Our Target Variable\n",
    "\n",
    "We want to predict: **`points`** — the number of points a driver scores in a race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at our target variable\n",
    "print(\"TARGET VARIABLE: points\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Data type: {df['points'].dtype}\")\n",
    "print(f\"Unique values: {sorted(df['points'].unique())}\")\n",
    "print(f\"Min: {df['points'].min()}, Max: {df['points'].max()}\")\n",
    "print(f\"Mean: {df['points'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-regression-explanation",
   "metadata": {},
   "source": [
    "### Three Types of ML Problems\n",
    "\n",
    "| Problem Type | What You Predict | Example Question | Models |\n",
    "|--------------|------------------|------------------|--------|\n",
    "| **Regression** | A continuous NUMBER | \"How many points will driver score?\" | Linear Regression, Decision Tree Regressor |\n",
    "| **Classification** | A CATEGORY/CLASS | \"Will driver win? (Yes/No)\" | Logistic Regression, Decision Tree Classifier |\n",
    "| **Clustering** | GROUPS (no target) | \"Which drivers are similar?\" | K-Means, DBSCAN |\n",
    "\n",
    "### Our Choice: REGRESSION\n",
    "\n",
    "**Why?**\n",
    "\n",
    "Our target `points` is a **continuous numerical value**: 0, 1, 2, 4, 6, 8, 10, 12, 15, 18, 25, 26\n",
    "\n",
    "- We are predicting a **NUMBER**, not a category\n",
    "- We want to know **how many** points, not just \"points or no points\"\n",
    "- The prediction can be any value in a range\n",
    "\n",
    "**If we wanted Classification, we would ask:**\n",
    "- \"Will driver finish on podium?\" (Yes/No) → Binary Classification\n",
    "- \"Will driver finish P1, P2-3, P4-10, or P11+?\" → Multi-class Classification\n",
    "\n",
    "**If we wanted Clustering, we would ask:**\n",
    "- \"Which drivers have similar performance patterns?\" → No target variable\n",
    "\n",
    "### Models We Will Use (as covered in class)\n",
    "\n",
    "For Regression problems, professor taught us to use:\n",
    "1. **Linear Regression** — Assumes linear relationship between features and target\n",
    "2. **Decision Tree Regressor** — Can capture non-linear patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Feature Selection <a id='4-feature-selection'></a>\n",
    "\n",
    "We select features that can help predict race points:\n",
    "\n",
    "| Feature | Type | Description | Why It Might Help |\n",
    "|---------|------|-------------|-------------------|\n",
    "| `grid_clean` | Numeric | Qualifying position | Better qualifying = more points |\n",
    "| `driver_avg_points_past` | Numeric | Driver's historical avg points | Good drivers score more |\n",
    "| `driver_consistency_past` | Numeric | Std dev of driver's past finishes | Consistent drivers are reliable |\n",
    "| `driver_races_past` | Numeric | Driver experience (race count) | Experience might help |\n",
    "| `constructor_strength_past` | Numeric | Team's historical avg points | Good car = more points |\n",
    "| `constructor_races_past` | Numeric | Team experience | Established teams perform better |\n",
    "| `constructorName` | Categorical | Team name | Team-specific effects |\n",
    "\n",
    "**Target:** `points` (0-26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "NUMERIC_FEATURES = [\n",
    "    \"grid_clean\",                  # Qualifying position\n",
    "    \"driver_avg_points_past\",      # Driver historical performance\n",
    "    \"driver_consistency_past\",     # Driver reliability (std of finishes)\n",
    "    \"driver_races_past\",           # Driver experience\n",
    "    \"constructor_strength_past\",   # Team historical performance\n",
    "    \"constructor_races_past\",      # Team experience\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"constructorName\",             # Team name (will be one-hot encoded)\n",
    "]\n",
    "\n",
    "TARGET = \"points\"\n",
    "\n",
    "# Combine all features\n",
    "ALL_FEATURES = NUMERIC_FEATURES + CATEGORICAL_FEATURES\n",
    "\n",
    "print(\"Feature Selection:\")\n",
    "print(f\"   Numeric features:     {len(NUMERIC_FEATURES)}\")\n",
    "print(f\"   Categorical features: {len(CATEGORICAL_FEATURES)}\")\n",
    "print(f\"   Target variable:      {TARGET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-model-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create modeling dataframe with selected features + target + year (for splitting)\n",
    "required_cols = ALL_FEATURES + [TARGET, \"year\"]\n",
    "df_model = df[required_cols].copy()\n",
    "\n",
    "print(f\"Modeling dataframe shape: {df_model.shape}\")\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Train/Test Split <a id='5-split'></a>\n",
    "\n",
    "We use **time-based split**:\n",
    "- **Training:** 2018-2022 (5 seasons)\n",
    "- **Testing:** 2023-2024 (2 seasons)\n",
    "\n",
    "This is better than random split because it simulates real-world prediction (using past to predict future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-test-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split\n",
    "X_train, X_test, y_train, y_test = time_based_split(\n",
    "    df_model,\n",
    "    feature_cols=ALL_FEATURES,\n",
    "    target_col=TARGET,\n",
    "    year_col=\"year\",\n",
    "    train_years=(2018, 2022),\n",
    "    test_years=(2023, 2024),\n",
    ")\n",
    "\n",
    "print(\"Train/Test Split (Time-Based):\")\n",
    "print(f\"   Training set: {len(X_train):,} rows (2018-2022)\")\n",
    "print(f\"   Test set:     {len(X_test):,} rows (2023-2024)\")\n",
    "print(f\"   Split ratio:  {len(X_train)/(len(X_train)+len(X_test))*100:.1f}% / {len(X_test)/(len(X_train)+len(X_test))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Build Preprocessor & Models <a id='6-build-models'></a>\n",
    "\n",
    "### Preprocessing Pipeline\n",
    "\n",
    "| Feature Type | Preprocessing Steps |\n",
    "|--------------|--------------------|\n",
    "| Numeric | Fill missing with median |\n",
    "| Categorical | Fill missing with most frequent, then One-Hot Encode |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-preprocessor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build preprocessor\n",
    "preprocessor = build_preprocessor(NUMERIC_FEATURES, CATEGORICAL_FEATURES)\n",
    "\n",
    "print(\"Preprocessor built!\")\n",
    "print(\"   - Numeric: MedianImputer\")\n",
    "print(\"   - Categorical: MostFrequentImputer + OneHotEncoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build models as pipelines\n",
    "models = {\n",
    "    \"LinearRegression\": Pipeline([\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", LinearRegression()),\n",
    "    ]),\n",
    "    \"DecisionTree\": Pipeline([\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"model\", DecisionTreeRegressor(random_state=42, max_depth=8)),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Models created:\")\n",
    "for name in models:\n",
    "    print(f\"   - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Train & Evaluate <a id='7-train-evaluate'></a>\n",
    "\n",
    "Train both models and evaluate using:\n",
    "- **R² (R-squared)** — How much variance is explained (higher is better, max 1.0)\n",
    "- **MSE (Mean Squared Error)** — Average squared error (lower is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each model\n",
    "results = []\n",
    "predictions = {}\n",
    "\n",
    "for name, pipeline in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    predictions[name] = y_pred\n",
    "    \n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"R2\": r2,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "    })\n",
    "    \n",
    "    print(f\"   R² = {r2:.4f}, MSE = {mse:.2f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table\n",
    "results_df = pd.DataFrame(results).sort_values(\"R2\", ascending=False)\n",
    "\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Tune Decision Tree <a id='8-tune-dt'></a>\n",
    "\n",
    "The Decision Tree might not be performing well with `max_depth=8`. Let's try different values to find the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tune-decision-tree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different max_depth values\n",
    "depth_results = []\n",
    "\n",
    "for depth in [3, 5, 8, 10, 15, None]:  # None = unlimited depth\n",
    "    # Build model with this depth\n",
    "    dt_pipeline = Pipeline([\n",
    "        (\"preprocess\", build_preprocessor(NUMERIC_FEATURES, CATEGORICAL_FEATURES)),\n",
    "        (\"model\", DecisionTreeRegressor(random_state=42, max_depth=depth)),\n",
    "    ])\n",
    "    \n",
    "    # Train and evaluate\n",
    "    dt_pipeline.fit(X_train, y_train)\n",
    "    y_pred = dt_pipeline.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    depth_results.append({\n",
    "        \"max_depth\": str(depth) if depth else \"None (unlimited)\",\n",
    "        \"R2\": r2,\n",
    "        \"MSE\": mse,\n",
    "    })\n",
    "\n",
    "depth_df = pd.DataFrame(depth_results).sort_values(\"R2\", ascending=False)\n",
    "\n",
    "print(\"DECISION TREE TUNING - Different max_depth values\")\n",
    "print(\"=\" * 60)\n",
    "display(depth_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best depth\n",
    "best_depth_row = depth_df.iloc[0]\n",
    "print(f\"Best max_depth: {best_depth_row['max_depth']}\")\n",
    "print(f\"Best R²: {best_depth_row['R2']:.4f}\")\n",
    "print(f\"Best MSE: {best_depth_row['MSE']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retrain-best-dt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain with best depth and update results\n",
    "best_depth_value = depth_df.iloc[0]['max_depth']\n",
    "if best_depth_value == \"None (unlimited)\":\n",
    "    best_depth_int = None\n",
    "else:\n",
    "    best_depth_int = int(best_depth_value)\n",
    "\n",
    "# Rebuild Decision Tree with best depth\n",
    "models[\"DecisionTree\"] = Pipeline([\n",
    "    (\"preprocess\", build_preprocessor(NUMERIC_FEATURES, CATEGORICAL_FEATURES)),\n",
    "    (\"model\", DecisionTreeRegressor(random_state=42, max_depth=best_depth_int)),\n",
    "])\n",
    "\n",
    "models[\"DecisionTree\"].fit(X_train, y_train)\n",
    "predictions[\"DecisionTree\"] = models[\"DecisionTree\"].predict(X_test)\n",
    "\n",
    "# Update results\n",
    "dt_r2 = r2_score(y_test, predictions[\"DecisionTree\"])\n",
    "dt_mse = mean_squared_error(y_test, predictions[\"DecisionTree\"])\n",
    "\n",
    "print(f\"Decision Tree retrained with max_depth={best_depth_int}\")\n",
    "print(f\"   New R² = {dt_r2:.4f}\")\n",
    "print(f\"   New MSE = {dt_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison after tuning\n",
    "final_results = []\n",
    "\n",
    "for name, pipeline in models.items():\n",
    "    y_pred = predictions[name]\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    final_results.append({\n",
    "        \"Model\": name,\n",
    "        \"R2\": r2,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "    })\n",
    "\n",
    "final_results_df = pd.DataFrame(final_results).sort_values(\"R2\", ascending=False)\n",
    "\n",
    "print(\"FINAL MODEL COMPARISON (after tuning)\")\n",
    "print(\"=\" * 60)\n",
    "display(final_results_df)\n",
    "\n",
    "# Determine winner\n",
    "best_model = final_results_df.iloc[0]['Model']\n",
    "best_r2 = final_results_df.iloc[0]['R2']\n",
    "best_mse = final_results_df.iloc[0]['MSE']\n",
    "\n",
    "print(f\"\\nWINNER: {best_model}\")\n",
    "print(f\"   R² = {best_r2:.4f} ({best_r2*100:.1f}% variance explained)\")\n",
    "print(f\"   MSE = {best_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_csv_path = REPORT_DIR / \"model_results.csv\"\n",
    "final_results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"Results saved to: {results_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Results Visualization <a id='9-visualization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-r2-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R² Comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "colors = ['#2ecc71' if m == best_model else '#3498db' for m in final_results_df['Model']]\n",
    "bars = ax.bar(final_results_df['Model'], final_results_df['R2'], color=colors, edgecolor='black')\n",
    "\n",
    "ax.set_title('Model Comparison: R² Score (Higher is Better)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('R² Score')\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, final_results_df['R2']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "            f'{val:.3f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"ml_model_r2_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIG_DIR / 'ml_model_r2_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-mse-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE Comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "colors = ['#2ecc71' if m == best_model else '#e74c3c' for m in final_results_df['Model']]\n",
    "bars = ax.bar(final_results_df['Model'], final_results_df['MSE'], color=colors, edgecolor='black')\n",
    "\n",
    "ax.set_title('Model Comparison: MSE (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Mean Squared Error')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, final_results_df['MSE']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "            f'{val:.2f}', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"ml_model_mse_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIG_DIR / 'ml_model_mse_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-predicted-vs-actual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual for best model\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "y_pred_best = predictions[best_model]\n",
    "\n",
    "ax.scatter(y_test, y_pred_best, alpha=0.5, color='#3498db', edgecolors='white', s=50)\n",
    "\n",
    "# Perfect prediction line\n",
    "max_val = max(y_test.max(), y_pred_best.max())\n",
    "ax.plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "\n",
    "ax.set_title(f'Predicted vs Actual Points ({best_model})', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Actual Points')\n",
    "ax.set_ylabel('Predicted Points')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"ml_predicted_vs_actual.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIG_DIR / 'ml_predicted_vs_actual.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-residuals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "residuals = y_test.values - y_pred_best\n",
    "\n",
    "ax.scatter(y_pred_best, residuals, alpha=0.5, color='#9b59b6', edgecolors='white', s=50)\n",
    "ax.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "ax.set_title(f'Residuals Analysis ({best_model})', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Predicted Points')\n",
    "ax.set_ylabel('Residual (Actual - Predicted)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"ml_residuals_analysis.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIG_DIR / 'ml_residuals_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Feature Importance <a id='10-feature-importance'></a>\n",
    "\n",
    "Decision Tree can tell us which features are most important for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after preprocessing\n",
    "dt_model = models[\"DecisionTree\"]\n",
    "preprocessor_fitted = dt_model.named_steps['preprocess']\n",
    "\n",
    "# Get feature names\n",
    "num_features = NUMERIC_FEATURES\n",
    "cat_features = list(preprocessor_fitted.named_transformers_['cat']\n",
    "                   .named_steps['onehot']\n",
    "                   .get_feature_names_out(CATEGORICAL_FEATURES))\n",
    "\n",
    "all_feature_names = num_features + cat_features\n",
    "\n",
    "# Get importances\n",
    "importances = dt_model.named_steps['model'].feature_importances_\n",
    "\n",
    "# Create dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': all_feature_names,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importances (Decision Tree):\")\n",
    "display(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance (top 15)\n",
    "top_n = 15\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = ['#e74c3c' if imp > 0.1 else '#3498db' for imp in top_features['importance']]\n",
    "\n",
    "bars = ax.barh(range(len(top_features)), top_features['importance'], color=colors, edgecolor='black')\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'])\n",
    "ax.invert_yaxis()  # Highest at top\n",
    "\n",
    "ax.set_title('Feature Importance - Decision Tree', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_ylabel('Feature')\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, imp) in enumerate(zip(bars, top_features['importance'])):\n",
    "    ax.text(imp + 0.005, i, f'{imp:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / \"ml_feature_importance.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {FIG_DIR / 'ml_feature_importance.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Conclusions <a id='11-conclusions'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MACHINE LEARNING RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"PROBLEM TYPE: Regression\")\n",
    "print(\"   - Target: points (continuous numerical value 0-26)\")\n",
    "print(\"   - Why Regression? We predict a NUMBER, not a category\")\n",
    "print()\n",
    "print(\"DATASET:\")\n",
    "print(f\"   Total records:     {len(df_model):,}\")\n",
    "print(f\"   Training set:      {len(X_train):,} (2018-2022)\")\n",
    "print(f\"   Test set:          {len(X_test):,} (2023-2024)\")\n",
    "print(f\"   Features used:     {len(ALL_FEATURES)} ({len(NUMERIC_FEATURES)} numeric, {len(CATEGORICAL_FEATURES)} categorical)\")\n",
    "print()\n",
    "print(\"MODEL COMPARISON:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Model':<25} {'R²':>10} {'MSE':>10}\")\n",
    "print(\"-\" * 70)\n",
    "for _, row in final_results_df.iterrows():\n",
    "    marker = \" <-- WINNER\" if row['Model'] == best_model else \"\"\n",
    "    print(f\"{row['Model']:<25} {row['R2']:>10.4f} {row['MSE']:>10.2f}{marker}\")\n",
    "print(\"-\" * 70)\n",
    "print()\n",
    "print(f\"WINNER: {best_model}\")\n",
    "print(f\"   - R² = {best_r2:.4f} (explains {best_r2*100:.1f}% of variance)\")\n",
    "print(f\"   - MSE = {best_mse:.2f}\")\n",
    "print()\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-outputs",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list generated files\n",
    "print(\"Generated Files:\")\n",
    "print()\n",
    "print(\"Results:\")\n",
    "print(f\"   - {results_csv_path}\")\n",
    "print()\n",
    "print(\"Visualizations:\")\n",
    "for fig_file in sorted(FIG_DIR.glob(\"ml_*.png\")):\n",
    "    print(f\"   - {fig_file.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
